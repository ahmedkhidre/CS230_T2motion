{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6c4778",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01def79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akhidre/pubgit/.My_HumanML3D/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Users/akhidre/pubgit/.My_HumanML3D/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras import mixed_precision\n",
    "#from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib.animation import FuncAnimation \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import copy\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4ddefa",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d19b7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "from_scratch=False # True: start fresh, False: resume from checkpoint\n",
    "\n",
    "# Model paths\n",
    "SAVE_Full_MODEL_PATH = f\"/Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_{time_stamp}.keras\" # saving full model checkpoint if from scratch=True\n",
    "LOAD_MODEL_PATH = \"/Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\"  # path to load model checkpoint from if from_scratch=False\n",
    "\n",
    "# TensorBoard directory\n",
    "log_dir = f\"/Users/akhidre/pubgit/CS230_T2motion/fit/{time_stamp}/\"\n",
    "\n",
    "#define data destination\n",
    "TRAIN_NPZ = \"/Users/akhidre/pubgit/HumanML3D/HumanML3D/paired_text_motion.npz\"\n",
    "TEST_NPZ  = \"/Users/akhidre/pubgit/HumanML3D/HumanML3D/paired_text_motion_val.npz\"\n",
    "NORM_STATS = \"motion_norm_stats.npz\"   # if exists, used; otherwise computed from train npz\n",
    "\n",
    "MOTION_LEN = 200         # fixed length for MLP outputs (frames)\n",
    "NUM_JOINTS = 22\n",
    "COORDS = 3\n",
    "OUTPUT_DIM = MOTION_LEN * NUM_JOINTS * COORDS\n",
    "root_trajectory = False  # whether to reconstruct motion root trajectory from root velocities\n",
    "\n",
    "# Data loading options\n",
    "MAX_TRAIN_SAMPLES =0    # 0 = use all; otherwise use first N pairs\n",
    "MAX_TEST_SAMPLES = 0   # 0 = use all; otherwise use first N pairs\n",
    "\n",
    "USE_NORMALIZATION = True  #normalize data to zero mean and unity variance\n",
    "\n",
    "# Training Monitoring and Callbacks\n",
    "USE_GPU = False\n",
    "USE_LR_SCHEDULER = False\n",
    "USE_EARLY_STOPPING = True\n",
    "USE_LR_LOGGER = False # learning rate logger callback\n",
    "\n",
    "# Training hyperparams\n",
    "D_TIME = 32   # Dimension of learnable time embeddings (e.g. 16/32/64)\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "HIDDEN_DIMS = [256,256]  # list: number of neurons per hidden Dense layer\n",
    "\n",
    "# Loss options\n",
    "USE_VELOCITY_LOSS = True\n",
    "LAMBDA_VEL = 1\n",
    "LAMBDA_ROOT = 1.0  # weight for root-velocity loss vs pose loss (tune if needed)\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736e8471",
   "metadata": {},
   "source": [
    "### Utilities Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86335104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_pairs(npz_path, max_samples=0):\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    z_texts = data[\"z_texts\"]   # shape (N, 384)\n",
    "    motions = data[\"motions\"]   # dtype=object, each entry (T, J, 3)\n",
    "    motion_ids = data[\"motion_ids\"] if \"motion_ids\" in data.files else None\n",
    "\n",
    "    if max_samples and max_samples > 0:\n",
    "        z_texts = z_texts[:max_samples]\n",
    "        motions = motions[:max_samples]\n",
    "        if motion_ids is not None:\n",
    "            motion_ids = motion_ids[:max_samples]\n",
    "\n",
    "    return z_texts, motions, motion_ids\n",
    "\n",
    "def filter_valid_motions(z_list, motions_list, ids_list=None,\n",
    "                         num_joints=22, coords=3):\n",
    "    \"\"\"\n",
    "    Filters out invalid motion sequences that do not match (T, num_joints, coords).\n",
    "\n",
    "    Returns:\n",
    "        valid_z_list, valid_motions_list, valid_ids_list (or None if no ids)\n",
    "    \"\"\"\n",
    "    valid_z = []\n",
    "    valid_motions = []\n",
    "    valid_ids = [] if ids_list is not None else None\n",
    "\n",
    "    for idx, (z, m) in enumerate(zip(z_list, motions_list)):\n",
    "        arr = np.array(m)\n",
    "\n",
    "        # Check dimensionality\n",
    "        if arr.ndim != 3:\n",
    "            print(f\"[Filter] Skipping sample at index={idx}, shape={arr.shape} (not 3D)\")\n",
    "            continue\n",
    "\n",
    "        # Check joint and coordinate dimensions\n",
    "        if arr.shape[1] != num_joints or arr.shape[2] != coords:\n",
    "            print(f\"[Filter] Skipping sample at index={idx}, shape={arr.shape} (bad joint dims)\")\n",
    "            continue\n",
    "\n",
    "        # Keep sample\n",
    "        valid_z.append(z)\n",
    "        valid_motions.append(arr)\n",
    "\n",
    "        if ids_list is not None:\n",
    "            valid_ids.append(ids_list[idx])\n",
    "\n",
    "    if ids_list is not None:\n",
    "        return np.array(valid_z, dtype=np.float32), valid_motions, valid_ids\n",
    "    else:\n",
    "        return np.array(valid_z, dtype=np.float32), valid_motions, None\n",
    "\n",
    "def pad_or_truncate_motion(motion, target_len=MOTION_LEN):\n",
    "    \"\"\"\n",
    "    motion: (T, J, 3) -> returns (target_len, J, 3)\n",
    "    \"\"\"\n",
    "    T, J, C = motion.shape\n",
    "    motion = motion.astype(np.float32)\n",
    "    if T == target_len:\n",
    "        return motion\n",
    "    if T > target_len:\n",
    "        return motion[:target_len]\n",
    "    # T < target_len: pad by repeating last frame\n",
    "    pad_len = target_len - T\n",
    "    last_frame = motion[-1][None, :, :]  # (1, J, 3)\n",
    "    pad = np.repeat(last_frame, pad_len, axis=0)\n",
    "    return np.concatenate([motion, pad], axis=0)\n",
    "\n",
    "\n",
    "def extract_pose_and_rootvel(motion, target_len=MOTION_LEN):\n",
    "    \"\"\"\n",
    "    motion: (T, NUM_JOINTS, 3) in absolute/world coordinates.\n",
    "    Returns:\n",
    "      pose_rel: (T, NUM_JOINTS, 3)  -> root-relative pose\n",
    "      root_vel: (T, 3)              -> root velocity per frame\n",
    "    \"\"\"\n",
    "    motion_fixed = pad_or_truncate_motion(motion, target_len)  # (T,J,3)\n",
    "\n",
    "    # global root per frame = joint 0\n",
    "    root = motion_fixed[:, 0, :]                               # (T,3)\n",
    "    pose_rel = motion_fixed - motion_fixed[:, 0:1, :]          # (T,J,3)\n",
    "\n",
    "    # root velocities (Î”root); v[0] = 0\n",
    "    root_vel = np.zeros_like(root)\n",
    "    root_vel[1:] = root[1:] - root[:-1]\n",
    "    return pose_rel, root_vel\n",
    "\n",
    "\n",
    "def compute_pose_rootvel_stats(motions, target_len=MOTION_LEN, save_path=None):\n",
    "    \"\"\"\n",
    "    Compute mean/std for:\n",
    "      - root-relative pose (per coord)\n",
    "      - root velocities (per coord)\n",
    "\n",
    "    motions: list of (T, J, 3) arrays (TRAIN set)\n",
    "    Returns:\n",
    "      pose_mean:     (1,1,1,3)\n",
    "      pose_std:      (1,1,1,3)\n",
    "      rootvel_mean:  (1,1,3)\n",
    "      rootvel_std:   (1,1,3)\n",
    "    \"\"\"\n",
    "    pose_coords = []\n",
    "    rootvel_coords = []\n",
    "\n",
    "    for m in motions:\n",
    "        pose_rel, root_vel = extract_pose_and_rootvel(m, target_len)\n",
    "        pose_coords.append(pose_rel.reshape(-1, COORDS))    # (T*J,3)\n",
    "        rootvel_coords.append(root_vel.reshape(-1, COORDS)) # (T,3)\n",
    "\n",
    "    pose_coords = np.concatenate(pose_coords, axis=0)\n",
    "    rootvel_coords = np.concatenate(rootvel_coords, axis=0)\n",
    "\n",
    "    pose_mean = pose_coords.mean(axis=0, keepdims=True)         # (1,3)\n",
    "    pose_std  = pose_coords.std(axis=0, keepdims=True) + 1e-8\n",
    "    rootvel_mean = rootvel_coords.mean(axis=0, keepdims=True)   # (1,3)\n",
    "    rootvel_std  = rootvel_coords.std(axis=0, keepdims=True) + 1e-8\n",
    "\n",
    "    # reshape for broadcasting\n",
    "    pose_mean    = pose_mean.reshape(1, 1, 1, 3).astype(np.float32)\n",
    "    pose_std     = pose_std.reshape(1, 1, 1, 3).astype(np.float32)\n",
    "    rootvel_mean = rootvel_mean.reshape(1, 1, 3).astype(np.float32)\n",
    "    rootvel_std  = rootvel_std.reshape(1, 1, 3).astype(np.float32)\n",
    "\n",
    "    if save_path is not None:\n",
    "        np.savez(\n",
    "            save_path,\n",
    "            pose_mean=pose_mean,\n",
    "            pose_std=pose_std,\n",
    "            rootvel_mean=rootvel_mean,\n",
    "            rootvel_std=rootvel_std,\n",
    "        )\n",
    "        print(f\"Saved pose/rootvel normalization stats to {save_path}\")\n",
    "\n",
    "    return pose_mean, pose_std, rootvel_mean, rootvel_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c590552",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2849de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train npz: /Users/akhidre/pubgit/HumanML3D/HumanML3D/paired_text_motion.npz\n",
      "Filtering training motions...\n",
      "[Filter] Skipping sample at index=2384, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=2385, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=2386, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=14019, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=14020, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=14021, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=37332, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=37333, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=37334, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=48967, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=48968, shape=(22, 3) (not 3D)\n",
      "[Filter] Skipping sample at index=48969, shape=(22, 3) (not 3D)\n",
      "Final train samples: 69884\n",
      "Loading test npz: /Users/akhidre/pubgit/HumanML3D/HumanML3D/paired_text_motion_val.npz\n",
      "Filtering test motions...\n",
      "Final test samples: 4362\n",
      "Train captions: (69884, 384)\n",
      "Train motions count: 69884\n",
      "Train ids_count: 69884\n",
      "Test captions: (4362, 384)\n",
      "Test motions count: 4362\n",
      "Test ids_count: 4362\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading train npz:\", TRAIN_NPZ)\n",
    "z_train, motions_train, ids_train = load_npz_pairs(TRAIN_NPZ, max_samples=MAX_TRAIN_SAMPLES)\n",
    "print(\"Filtering training motions...\")\n",
    "z_train, motions_train, ids_train = filter_valid_motions(z_train, motions_train, ids_train)\n",
    "print(\"Final train samples:\", len(z_train))\n",
    "\n",
    "print(\"Loading test npz:\", TEST_NPZ)\n",
    "z_test, motions_test, ids_test = load_npz_pairs(TEST_NPZ, max_samples=MAX_TEST_SAMPLES)\n",
    "\n",
    "print(\"Filtering test motions...\")\n",
    "z_test, motions_test, ids_test = filter_valid_motions(z_test, motions_test, ids_test)\n",
    "print(\"Final test samples:\", len(z_test))\n",
    "\n",
    "\n",
    "print(\"Train captions:\", z_train.shape)\n",
    "print(\"Train motions count:\", len(motions_train))\n",
    "print(\"Train ids_count:\", len(ids_train))\n",
    "print(\"Test captions:\", z_test.shape)\n",
    "print(\"Test motions count:\", len(motions_test))\n",
    "print(\"Test ids_count:\", len(ids_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bfdcd",
   "metadata": {},
   "source": [
    "### Compute data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34216840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing normalization stats (root-relative pose + root velocity)...\n",
      "pose_mean: [[[[-0.00023493 -0.02042538  0.01167196]]]] pose_std: [[[[0.1076361  0.23476106 0.1065274 ]]]]\n",
      "rootvel_mean: [[[ 3.9163851e-06 -8.9967598e-06  2.7644455e-03]]] rootvel_std: [[[0.01206515 0.0096471  0.01815101]]]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(NORM_STATS):\n",
    "    stats = np.load(NORM_STATS)\n",
    "    pose_mean     = stats[\"pose_mean\"]\n",
    "    pose_std      = stats[\"pose_std\"]\n",
    "    rootvel_mean  = stats[\"rootvel_mean\"]\n",
    "    rootvel_std   = stats[\"rootvel_std\"]\n",
    "    print(\"Loaded normalization stats from\", NORM_STATS)\n",
    "else:\n",
    "    print(\"Computing normalization stats (root-relative pose + root velocity)...\")\n",
    "    pose_mean, pose_std, rootvel_mean, rootvel_std = compute_pose_rootvel_stats(\n",
    "        motions_train,\n",
    "        target_len=MOTION_LEN,\n",
    "        save_path=None\n",
    "    )\n",
    "\n",
    "print(\"pose_mean:\", pose_mean, \"pose_std:\", pose_std)\n",
    "print(\"rootvel_mean:\", rootvel_mean, \"rootvel_std:\", rootvel_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e8e57e",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e720518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training tensors...\n",
      "Preparing test tensors...\n",
      "Shapes:\n",
      "  X_train: (69884, 384)\n",
      "  Y_pose_train: (69884, 200, 22, 3)\n",
      "  Y_root_train: (69884, 200, 3)\n",
      "  X_test: (4362, 384)\n",
      "  Y_pose_test: (4362, 200, 22, 3)\n",
      "  Y_root_test: (4362, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "def prepare_xy_multi(z_list,\n",
    "                     motions_objectlist,\n",
    "                     pose_mean, pose_std,\n",
    "                     rootvel_mean, rootvel_std,\n",
    "                     motion_len=MOTION_LEN,use_normalization=USE_NORMALIZATION):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X:       (N, 384)\n",
    "      Y_pose:  (N, T, J, 3)   normalized root-relative pose\n",
    "      Y_root:  (N, T, 3)      normalized root velocities\n",
    "    \"\"\"\n",
    "    N = len(z_list)\n",
    "    X = np.array(z_list, dtype=np.float32)  # (N, 384)\n",
    "\n",
    "    Y_pose = np.zeros((N, motion_len, NUM_JOINTS, COORDS), dtype=np.float32)\n",
    "    Y_root = np.zeros((N, motion_len, COORDS), dtype=np.float32)\n",
    "\n",
    "    for i, m in enumerate(motions_objectlist):\n",
    "        pose_rel, root_vel = extract_pose_and_rootvel(m, target_len=motion_len)\n",
    "\n",
    "        pose_norm    = (pose_rel - pose_mean) / pose_std\n",
    "        rootvel_norm = (root_vel - rootvel_mean) / rootvel_std\n",
    "        if use_normalization:\n",
    "            Y_pose[i] = pose_norm\n",
    "            Y_root[i] = rootvel_norm\n",
    "        else:\n",
    "            Y_pose[i] = pose_rel\n",
    "            Y_root[i] = root_vel\n",
    "\n",
    "    return X, Y_pose, Y_root\n",
    "\n",
    "print(\"Preparing training tensors...\")\n",
    "X_train, Y_pose_train, Y_root_train = prepare_xy_multi(\n",
    "    z_train, motions_train,\n",
    "    pose_mean, pose_std,\n",
    "    rootvel_mean, rootvel_std,\n",
    "    motion_len=MOTION_LEN,\n",
    "    use_normalization=USE_NORMALIZATION\n",
    ")\n",
    "\n",
    "print(\"Preparing test tensors...\")\n",
    "X_test, Y_pose_test, Y_root_test = prepare_xy_multi(\n",
    "    z_test, motions_test,\n",
    "    pose_mean, pose_std,\n",
    "    rootvel_mean, rootvel_std,\n",
    "    motion_len=MOTION_LEN,\n",
    "    use_normalization=USE_NORMALIZATION\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  Y_pose_train:\", Y_pose_train.shape)\n",
    "print(\"  Y_root_train:\", Y_root_train.shape)\n",
    "print(\"  X_test:\", X_test.shape)\n",
    "print(\"  Y_pose_test:\", Y_pose_test.shape)\n",
    "print(\"  Y_root_test:\", Y_root_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702d03a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d010d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GRU_TimeCond_Motion_Decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " z_text (InputLayer)         [(None, 384)]                0         []                            \n",
      "                                                                                                  \n",
      " time_indices (Lambda)       (None, 200)                  0         ['z_text[0][0]']              \n",
      "                                                                                                  \n",
      " repeat_text (RepeatVector)  (None, 200, 384)             0         ['z_text[0][0]']              \n",
      "                                                                                                  \n",
      " time_embedding (Embedding)  (None, 200, 32)              6400      ['time_indices[0][0]']        \n",
      "                                                                                                  \n",
      " concat_text_time (Concaten  (None, 200, 416)             0         ['repeat_text[0][0]',         \n",
      " ate)                                                                'time_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " init_state_from_caption (D  (None, 256)                  98560     ['z_text[0][0]']              \n",
      " ense)                                                                                            \n",
      "                                                                                                  \n",
      " gru_layer_1 (GRU)           (None, 200, 256)             517632    ['concat_text_time[0][0]',    \n",
      "                                                                     'init_state_from_caption[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " gru_layer_2 (GRU)           (None, 200, 256)             394752    ['gru_layer_1[0][0]']         \n",
      "                                                                                                  \n",
      " pose_frame_dense (Dense)    (None, 200, 66)              16962     ['gru_layer_2[0][0]']         \n",
      "                                                                                                  \n",
      " pose_out (Reshape)          (None, 200, 22, 3)           0         ['pose_frame_dense[0][0]']    \n",
      "                                                                                                  \n",
      " rootvel_out (Dense)         (None, 200, 3)               771       ['gru_layer_2[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1035077 (3.95 MB)\n",
      "Trainable params: 1035077 (3.95 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()  # Clear any existing session\n",
    "\n",
    "# Optional mixed precision on GPU\n",
    "if USE_GPU:\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision policy:\", mixed_precision.global_policy())\n",
    "\n",
    "# ---------------------------\n",
    "# Inputs: sentence embedding z_text (384-d)\n",
    "# ---------------------------\n",
    "inputs = tf.keras.Input(\n",
    "    shape=(X_train.shape[1],),  # 384 for all-MiniLM\n",
    "    dtype=tf.float32,\n",
    "    name=\"z_text\"\n",
    ")\n",
    "\n",
    "CAP_DIM = X_train.shape[1]      # typically 384\n",
    "\n",
    "# 1) Repeat text embedding across time -> [B, T, CAP_DIM]\n",
    "z_seq = tf.keras.layers.RepeatVector(MOTION_LEN, name=\"repeat_text\")(inputs)\n",
    "\n",
    "# 2) Learnable time embedding indices [0..T-1]\n",
    "def make_time_indices(z):\n",
    "    batch_size = tf.shape(z)[0]\n",
    "    time_range = tf.range(MOTION_LEN, dtype=tf.int32)  # [T]\n",
    "    time_range = tf.expand_dims(time_range, axis=0)    # [1, T]\n",
    "    time_indices = tf.tile(time_range, [batch_size, 1])  # [B, T]\n",
    "    return time_indices\n",
    "\n",
    "time_indices = tf.keras.layers.Lambda(\n",
    "    make_time_indices,\n",
    "    name=\"time_indices\"\n",
    ")(inputs)  # [B, T]\n",
    "\n",
    "time_embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=MOTION_LEN,   # one embedding per frame index\n",
    "    output_dim=D_TIME,\n",
    "    name=\"time_embedding\"\n",
    ")\n",
    "\n",
    "time_seq = time_embedding_layer(time_indices)  # [B, T, D_TIME]\n",
    "\n",
    "# 3) Concatenate text + time -> [B, T, CAP_DIM + D_TIME]\n",
    "decoder_input = tf.keras.layers.Concatenate(\n",
    "    axis=-1,\n",
    "    name=\"concat_text_time\"\n",
    ")([z_seq, time_seq])\n",
    "\n",
    "# 4) Stacked GRU decoder over time using HIDDEN_DIMS\n",
    "\n",
    "# Learn initial hidden state from caption only\n",
    "init_state = tf.keras.layers.Dense(\n",
    "    HIDDEN_DIMS[0],           # same size as first GRU layer\n",
    "    activation=\"tanh\",\n",
    "    name=\"init_state_from_caption\"\n",
    ")(inputs)                     # inputs is z_text: (B, CAP_DIM)\n",
    "\n",
    "# First GRU layer uses learned initial_state\n",
    "x = tf.keras.layers.GRU(\n",
    "    HIDDEN_DIMS[0],\n",
    "    return_sequences=True,\n",
    "    name=\"gru_layer_1\"\n",
    ")(decoder_input, initial_state=init_state)\n",
    "\n",
    "# Additional GRU layers (if any) stack on top, default zero init is fine\n",
    "for i, h in enumerate(HIDDEN_DIMS[1:], start=2):\n",
    "    x = tf.keras.layers.GRU(\n",
    "        h,\n",
    "        return_sequences=True,\n",
    "        name=f\"gru_layer_{i}\"\n",
    "    )(x)\n",
    "\n",
    "\n",
    "# 5) Two heads:\n",
    "#    - pose_out:    (B, T, 22, 3)   root-relative pose\n",
    "#    - rootvel_out: (B, T, 3)       root velocities\n",
    "\n",
    "# Pose head\n",
    "pose_out_flat = tf.keras.layers.Dense(\n",
    "    NUM_JOINTS * COORDS,\n",
    "    name=\"pose_frame_dense\"\n",
    ")(x)                   # (B, T, 66)\n",
    "\n",
    "pose_out = tf.keras.layers.Reshape(\n",
    "    (MOTION_LEN, NUM_JOINTS, COORDS),\n",
    "    name=\"pose_out\"\n",
    ")(pose_out_flat)       # (B, T, 22, 3)\n",
    "\n",
    "# Root-velocity head\n",
    "rootvel_out = tf.keras.layers.Dense(\n",
    "    COORDS,\n",
    "    name=\"rootvel_out\"\n",
    ")(x)                   # (B, T, 3)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=[pose_out, rootvel_out],\n",
    "    name=\"GRU_TimeCond_Motion_Decoder\"\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ---------------------------\n",
    "# Losses\n",
    "# ---------------------------\n",
    "def pose_loss_with_vel(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    y_*: (B, T, J, 3)\n",
    "    MSE on pose + optional velocity loss on pose.\n",
    "    \"\"\"\n",
    "    pos_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "    if USE_VELOCITY_LOSS:\n",
    "        vel_true = y_true[:, 1:, :, :] - y_true[:, :-1, :, :]\n",
    "        vel_pred = y_pred[:, 1:, :, :] - y_pred[:, :-1, :, :]\n",
    "        vel_loss = tf.reduce_mean(tf.square(vel_true - vel_pred))\n",
    "        return pos_loss + LAMBDA_VEL * vel_loss\n",
    "    else:\n",
    "        return pos_loss\n",
    "\n",
    "\n",
    "optimizer = AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[pose_loss_with_vel, \"mse\"],      # [pose_out, rootvel_out]\n",
    "    loss_weights=[1.0, LAMBDA_ROOT]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c614f4",
   "metadata": {},
   "source": [
    "### Training callbacks and monitors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a42bb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,   # enables gradients & weights monitoring\n",
    "    write_graph=False\n",
    ")\n",
    "\n",
    "# Reduce LR on Plateau\n",
    "reduceLR_cb = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save best weights only\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=SAVE_Full_MODEL_PATH,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "class LRTensorBoard(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir):\n",
    "        super().__init__()\n",
    "        self.file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        with self.file_writer.as_default():\n",
    "            tf.summary.scalar(\"learning_rate\", lr, step=epoch)\n",
    "\n",
    "lr_logger = LRTensorBoard(log_dir=log_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "callbacks_list = [tensorboard_cb, checkpoint_cb]\n",
    "\n",
    "if USE_LR_SCHEDULER:\n",
    "    callbacks_list.append(reduceLR_cb)\n",
    "\n",
    "if USE_EARLY_STOPPING:\n",
    "    callbacks_list.append(earlystop_cb)\n",
    "\n",
    "# Optional: add LR logger if you use it\n",
    "if USE_LR_LOGGER:\n",
    "    callbacks_list.append(lr_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761c5a8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26f29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "Epoch 1/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7910 - pose_out_loss: 0.8290 - rootvel_out_loss: 0.9620\n",
      "Epoch 1: val_loss improved from 1.79111 to 1.78526, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 285s 4s/step - loss: 1.7910 - pose_out_loss: 0.8290 - rootvel_out_loss: 0.9620 - val_loss: 1.7853 - val_pose_out_loss: 0.8595 - val_rootvel_out_loss: 0.9257\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7848 - pose_out_loss: 0.8250 - rootvel_out_loss: 0.9598\n",
      "Epoch 2: val_loss improved from 1.78526 to 1.77926, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 283s 4s/step - loss: 1.7848 - pose_out_loss: 0.8250 - rootvel_out_loss: 0.9598 - val_loss: 1.7793 - val_pose_out_loss: 0.8546 - val_rootvel_out_loss: 0.9247\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7798 - pose_out_loss: 0.8209 - rootvel_out_loss: 0.9589\n",
      "Epoch 3: val_loss improved from 1.77926 to 1.77359, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 282s 4s/step - loss: 1.7798 - pose_out_loss: 0.8209 - rootvel_out_loss: 0.9589 - val_loss: 1.7736 - val_pose_out_loss: 0.8497 - val_rootvel_out_loss: 0.9239\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7750 - pose_out_loss: 0.8174 - rootvel_out_loss: 0.9576\n",
      "Epoch 4: val_loss improved from 1.77359 to 1.76841, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 273s 4s/step - loss: 1.7750 - pose_out_loss: 0.8174 - rootvel_out_loss: 0.9576 - val_loss: 1.7684 - val_pose_out_loss: 0.8455 - val_rootvel_out_loss: 0.9229\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7708 - pose_out_loss: 0.8141 - rootvel_out_loss: 0.9567\n",
      "Epoch 5: val_loss improved from 1.76841 to 1.76416, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 243s 4s/step - loss: 1.7708 - pose_out_loss: 0.8141 - rootvel_out_loss: 0.9567 - val_loss: 1.7642 - val_pose_out_loss: 0.8420 - val_rootvel_out_loss: 0.9222\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7671 - pose_out_loss: 0.8112 - rootvel_out_loss: 0.9559\n",
      "Epoch 6: val_loss improved from 1.76416 to 1.76108, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 219s 3s/step - loss: 1.7671 - pose_out_loss: 0.8112 - rootvel_out_loss: 0.9559 - val_loss: 1.7611 - val_pose_out_loss: 0.8379 - val_rootvel_out_loss: 0.9232\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7641 - pose_out_loss: 0.8089 - rootvel_out_loss: 0.9552\n",
      "Epoch 7: val_loss improved from 1.76108 to 1.75559, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 239s 3s/step - loss: 1.7641 - pose_out_loss: 0.8089 - rootvel_out_loss: 0.9552 - val_loss: 1.7556 - val_pose_out_loss: 0.8352 - val_rootvel_out_loss: 0.9204\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7613 - pose_out_loss: 0.8068 - rootvel_out_loss: 0.9545\n",
      "Epoch 8: val_loss improved from 1.75559 to 1.75204, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 239s 3s/step - loss: 1.7613 - pose_out_loss: 0.8068 - rootvel_out_loss: 0.9545 - val_loss: 1.7520 - val_pose_out_loss: 0.8318 - val_rootvel_out_loss: 0.9202\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7576 - pose_out_loss: 0.8041 - rootvel_out_loss: 0.9535\n",
      "Epoch 9: val_loss improved from 1.75204 to 1.75022, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 226s 3s/step - loss: 1.7576 - pose_out_loss: 0.8041 - rootvel_out_loss: 0.9535 - val_loss: 1.7502 - val_pose_out_loss: 0.8295 - val_rootvel_out_loss: 0.9207\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7551 - pose_out_loss: 0.8023 - rootvel_out_loss: 0.9528\n",
      "Epoch 10: val_loss did not improve from 1.75022\n",
      "69/69 [==============================] - 233s 3s/step - loss: 1.7551 - pose_out_loss: 0.8023 - rootvel_out_loss: 0.9528 - val_loss: 1.7504 - val_pose_out_loss: 0.8304 - val_rootvel_out_loss: 0.9200\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7533 - pose_out_loss: 0.8009 - rootvel_out_loss: 0.9524\n",
      "Epoch 11: val_loss improved from 1.75022 to 1.74535, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 228s 3s/step - loss: 1.7533 - pose_out_loss: 0.8009 - rootvel_out_loss: 0.9524 - val_loss: 1.7454 - val_pose_out_loss: 0.8263 - val_rootvel_out_loss: 0.9191\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7512 - pose_out_loss: 0.7994 - rootvel_out_loss: 0.9518\n",
      "Epoch 12: val_loss improved from 1.74535 to 1.74298, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 259s 4s/step - loss: 1.7512 - pose_out_loss: 0.7994 - rootvel_out_loss: 0.9518 - val_loss: 1.7430 - val_pose_out_loss: 0.8236 - val_rootvel_out_loss: 0.9194\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7485 - pose_out_loss: 0.7975 - rootvel_out_loss: 0.9510\n",
      "Epoch 13: val_loss improved from 1.74298 to 1.74065, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 223s 3s/step - loss: 1.7485 - pose_out_loss: 0.7975 - rootvel_out_loss: 0.9510 - val_loss: 1.7407 - val_pose_out_loss: 0.8228 - val_rootvel_out_loss: 0.9178\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7469 - pose_out_loss: 0.7964 - rootvel_out_loss: 0.9506\n",
      "Epoch 14: val_loss improved from 1.74065 to 1.73909, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 247s 4s/step - loss: 1.7469 - pose_out_loss: 0.7964 - rootvel_out_loss: 0.9506 - val_loss: 1.7391 - val_pose_out_loss: 0.8218 - val_rootvel_out_loss: 0.9173\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7454 - pose_out_loss: 0.7949 - rootvel_out_loss: 0.9505\n",
      "Epoch 15: val_loss improved from 1.73909 to 1.73569, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 240s 3s/step - loss: 1.7454 - pose_out_loss: 0.7949 - rootvel_out_loss: 0.9505 - val_loss: 1.7357 - val_pose_out_loss: 0.8186 - val_rootvel_out_loss: 0.9171\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7436 - pose_out_loss: 0.7936 - rootvel_out_loss: 0.9500\n",
      "Epoch 16: val_loss did not improve from 1.73569\n",
      "69/69 [==============================] - 244s 4s/step - loss: 1.7436 - pose_out_loss: 0.7936 - rootvel_out_loss: 0.9500 - val_loss: 1.7361 - val_pose_out_loss: 0.8182 - val_rootvel_out_loss: 0.9179\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7430 - pose_out_loss: 0.7934 - rootvel_out_loss: 0.9496\n",
      "Epoch 17: val_loss improved from 1.73569 to 1.73567, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 247s 4s/step - loss: 1.7430 - pose_out_loss: 0.7934 - rootvel_out_loss: 0.9496 - val_loss: 1.7357 - val_pose_out_loss: 0.8184 - val_rootvel_out_loss: 0.9172\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7406 - pose_out_loss: 0.7917 - rootvel_out_loss: 0.9488\n",
      "Epoch 18: val_loss improved from 1.73567 to 1.73089, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 236s 3s/step - loss: 1.7406 - pose_out_loss: 0.7917 - rootvel_out_loss: 0.9488 - val_loss: 1.7309 - val_pose_out_loss: 0.8145 - val_rootvel_out_loss: 0.9163\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7382 - pose_out_loss: 0.7899 - rootvel_out_loss: 0.9483\n",
      "Epoch 19: val_loss improved from 1.73089 to 1.73013, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 227s 3s/step - loss: 1.7382 - pose_out_loss: 0.7899 - rootvel_out_loss: 0.9483 - val_loss: 1.7301 - val_pose_out_loss: 0.8136 - val_rootvel_out_loss: 0.9165\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7370 - pose_out_loss: 0.7889 - rootvel_out_loss: 0.9481\n",
      "Epoch 20: val_loss improved from 1.73013 to 1.72862, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 229s 3s/step - loss: 1.7370 - pose_out_loss: 0.7889 - rootvel_out_loss: 0.9481 - val_loss: 1.7286 - val_pose_out_loss: 0.8132 - val_rootvel_out_loss: 0.9155\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7356 - pose_out_loss: 0.7877 - rootvel_out_loss: 0.9479\n",
      "Epoch 21: val_loss improved from 1.72862 to 1.72640, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 224s 3s/step - loss: 1.7356 - pose_out_loss: 0.7877 - rootvel_out_loss: 0.9479 - val_loss: 1.7264 - val_pose_out_loss: 0.8111 - val_rootvel_out_loss: 0.9153\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7341 - pose_out_loss: 0.7869 - rootvel_out_loss: 0.9472\n",
      "Epoch 22: val_loss improved from 1.72640 to 1.72524, saving model to /Users/akhidre/pubgit/CS230_T2motion/models/full_checkpoint_RNN_2_head_20251203-101146.keras\n",
      "69/69 [==============================] - 245s 4s/step - loss: 1.7341 - pose_out_loss: 0.7869 - rootvel_out_loss: 0.9472 - val_loss: 1.7252 - val_pose_out_loss: 0.8105 - val_rootvel_out_loss: 0.9147\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.7324 - pose_out_loss: 0.7856 - rootvel_out_loss: 0.9468\n",
      "Epoch 23: val_loss did not improve from 1.72524\n",
      "69/69 [==============================] - 258s 4s/step - loss: 1.7324 - pose_out_loss: 0.7856 - rootvel_out_loss: 0.9468 - val_loss: 1.7262 - val_pose_out_loss: 0.8104 - val_rootvel_out_loss: 0.9158\n",
      "Epoch 24/50\n",
      " 6/69 [=>............................] - ETA: 4:41 - loss: 1.7456 - pose_out_loss: 0.7970 - rootvel_out_loss: 0.9486"
     ]
    }
   ],
   "source": [
    "if not from_scratch:\n",
    "    if os.path.exists(LOAD_MODEL_PATH):\n",
    "        print(f\"Loading model from: {LOAD_MODEL_PATH}\")\n",
    "        \n",
    "        model = tf.keras.models.load_model(LOAD_MODEL_PATH,\n",
    "            custom_objects={'pose_loss_with_vel': pose_loss_with_vel,},\n",
    "            safe_mode=False\n",
    "        )\n",
    "    else:\n",
    "         raise FileNotFoundError(f\"Model file not found:\\n{LOAD_MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"Training from scratch\")\n",
    "\n",
    "tf.config.run_functions_eagerly(False)  # for performance\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    [Y_pose_train, Y_root_train],\n",
    "    validation_data=(X_test, [Y_pose_test, Y_root_test]),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Combined Loss plot\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['loss'], label='train_total_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_total_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Training / Validation Loss (Pose + RootVel)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Pose-only loss plot\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['pose_out_loss'], label='train_pose_loss')\n",
    "plt.plot(history.history['val_pose_out_loss'], label='val_pose_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Pose Loss')\n",
    "plt.title('Pose Head Loss (Position + Optional Pose Velocity)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Root-velocity-only loss plot\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['rootvel_out_loss'], label='train_rootvel_loss')\n",
    "plt.plot(history.history['val_rootvel_out_loss'], label='val_rootvel_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Root Velocity Loss (MSE)')\n",
    "plt.title('Root Velocity Head Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa89077a",
   "metadata": {},
   "source": [
    "### Choosing Model for Inferance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21dcd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model already in memory (just trained).\n"
     ]
    }
   ],
   "source": [
    "if from_scratch:\n",
    "    print(\"Using model already in memory (just trained).\")\n",
    "else:\n",
    "    print(f\"\\nLoading model from file:\\n{LOAD_MODEL_PATH}\\n\")\n",
    "\n",
    "    if not os.path.exists(LOAD_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model file not found:\\n{LOAD_MODEL_PATH}\")\n",
    "\n",
    "    model = tf.keras.models.load_model(LOAD_MODEL_PATH,\n",
    "        custom_objects={\n",
    "            'pose_loss_with_vel': pose_loss_with_vel,\n",
    "        },\n",
    "        safe_mode=False\n",
    "    )\n",
    "\n",
    "    print(\"Model loaded successfully for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb4dde",
   "metadata": {},
   "source": [
    "### Run Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "542c9de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test motion ID: 012698\n",
      "1/1 [==============================] - 0s 432ms/step\n",
      "Predicted motion (pose only) shape: (200, 22, 3)\n",
      "GT motion (pose only) shape: (200, 22, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0   # pick any index from test set\n",
    "\n",
    "motion_id = ids_test[sample_idx]\n",
    "print(\"Test motion ID:\", motion_id)\n",
    "\n",
    "# Input embedding\n",
    "z = X_test[sample_idx:sample_idx+1]   # (1, CAP_DIM)\n",
    "\n",
    "# Forward pass through model -> two heads\n",
    "pred_pose_out, pred_rootvel_out = model.predict(z)   # (1,T,J,3), (1,T,3)\n",
    "\n",
    "# Remove batch dimension\n",
    "pred_pose_out = pred_pose_out[0]       # (T,J,3)\n",
    "pred_rootvel_out = pred_rootvel_out[0] # (T,3)\n",
    "\n",
    "if USE_NORMALIZATION:\n",
    "    # denormalize root-relative pose\n",
    "    pred_pose_rel = (\n",
    "        pred_pose_out * pose_std[0, 0, 0, :] + pose_mean[0, 0, 0, :]\n",
    "    )  # (T,J,3)\n",
    "\n",
    "    # denormalize root velocities\n",
    "    pred_root_vel = (\n",
    "        pred_rootvel_out * rootvel_std[0, 0, :] + rootvel_mean[0, 0, :]\n",
    "    )  # (T,3)\n",
    "\n",
    "else:\n",
    "    # model already trained on raw values\n",
    "    pred_pose_rel = pred_pose_out       # (T,J,3) root-relative, unnormalized\n",
    "    pred_root_vel = pred_rootvel_out    # (T,3)   root velocity, unnormalized\n",
    "\n",
    "if root_trajectory:\n",
    "    # calculate root trajectory from root velocities\n",
    "    root_pred = np.zeros_like(pred_root_vel, dtype=np.float32)  # (T,3)\n",
    "    for t in range(1, MOTION_LEN):\n",
    "        root_pred[t] = root_pred[t - 1] + pred_root_vel[t]\n",
    "    root_pred_expanded = root_pred[:, None, :]                   # (T,1,3)\n",
    "    pred_world = pred_pose_rel + root_pred_expanded              # (T,22,3)\n",
    "    print(\"Predicted motion (world coords) shape:\", pred_world.shape)\n",
    "\n",
    "else:\n",
    "    #consider pose only\n",
    "    pred_world = pred_pose_rel\n",
    "    print(\"Predicted motion (pose only) shape:\", pred_world.shape)\n",
    "\n",
    "# GT motion (padded) for comparison\n",
    "if root_trajectory:\n",
    "    gt_motion_fixed = pad_or_truncate_motion(motions_test[sample_idx], MOTION_LEN)\n",
    "    print(\"GT motion (world coords) shape:\", gt_motion_fixed.shape)\n",
    "else:\n",
    "    gt_motion_fixed, _ = extract_pose_and_rootvel(motions_test[sample_idx], target_len=MOTION_LEN)\n",
    "    print(\"GT motion (pose only) shape:\", gt_motion_fixed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2f4eb",
   "metadata": {},
   "source": [
    "### Animation of Generated Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fc03f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated_RNN_2heads animation to: /Users/akhidre/pubgit/CS230_T2Motion/animations/generated_RNN_2heads_012698_20251203_113407.mp4\n",
      "Saved gt_fixed animation to: /Users/akhidre/pubgit/CS230_T2Motion/animations/gt_fixed_012698_20251203_113414.mp4\n"
     ]
    }
   ],
   "source": [
    "# SMPL 22-joint skeleton edges\n",
    "edges = [\n",
    "    (0, 1), (1, 4), (4, 7), (7, 10),\n",
    "    (0, 2), (2, 5), (5, 8), (8, 11),\n",
    "    (0, 3), (3, 6), (6, 9), (9, 12), (12, 15),\n",
    "    (9, 13), (13, 16), (16, 18), (18, 20),\n",
    "    (9, 14), (14, 17), (17, 19), (19, 21),\n",
    "]\n",
    "\n",
    "def save_motion_animation(motion, label, motion_id, base_dir):\n",
    "    \"\"\"\n",
    "    motion: (T, 22, 3)\n",
    "    label:  'generated' or 'gt'\n",
    "    \"\"\"\n",
    "    pose = copy.deepcopy(motion)  # (T,22,3)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    # FIXED AXIS LIMITS \n",
    "    ax.set(xlim3d=(-1, 3), xlabel='X')\n",
    "    ax.set(ylim3d=(-1, 1), ylabel='Y')\n",
    "    ax.set(zlim3d=(-1, 1), zlabel='Z')\n",
    "\n",
    "\n",
    "    # Auto axis limits from motion\n",
    "    #coords = pose.reshape(-1, 3)\n",
    "    #x_min, x_max = coords[:, 0].min(), coords[:, 0].max()\n",
    "    #y_min, y_max = coords[:, 1].min(), coords[:, 1].max()\n",
    "    #z_min, z_max = coords[:, 2].min(), coords[:, 2].max()\n",
    "    #margin = 0.1\n",
    "\n",
    "    #ax.set_xlim(x_min - margin, x_max + margin)\n",
    "    #ax.set_ylim(y_min - margin, y_max + margin)\n",
    "    #ax.set_zlim(z_min - margin, z_max + margin)\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    # Create one line per edge\n",
    "    lines = [ax.plot([], [], [])[0] for _ in edges]\n",
    "\n",
    "    def update_lines(frame_num, pose, lines):\n",
    "        frame = pose[frame_num]\n",
    "        for n, (i, j) in enumerate(edges):\n",
    "            x = [frame[i, 0], frame[j, 0]]\n",
    "            y = [frame[i, 1], frame[j, 1]]\n",
    "            z = [frame[i, 2], frame[j, 2]]\n",
    "            # Keep your preferred orientation:\n",
    "            lines[n].set_data_3d([z, x, y])\n",
    "        return lines\n",
    "\n",
    "    ani = FuncAnimation(\n",
    "        fig,\n",
    "        update_lines,\n",
    "        frames=pose.shape[0],\n",
    "        fargs=(pose, lines),\n",
    "        interval=100\n",
    "    )\n",
    "\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_path = f\"{base_dir}/{label}_{motion_id}_{timestamp}.mp4\"\n",
    "    ani.save(save_path, writer='ffmpeg', fps=30)\n",
    "    print(f\"Saved {label} animation to:\", save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# save directory\n",
    "anim_dir = \"/Users/akhidre/pubgit/CS230_T2Motion/animations\"\n",
    "\n",
    "# Save predicted motion\n",
    "save_motion_animation(pred_world, \"generated_RNN_2heads\", motion_id, anim_dir)\n",
    "\n",
    "# Save GT motion\n",
    "save_motion_animation(gt_motion_fixed, \"gt_fixed\", motion_id, anim_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b32c77",
   "metadata": {},
   "source": [
    "### Static Frames of Generated Motions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2b4316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested frames: [1, 10, 100]\n",
      "Using frames: [1, 10, 100]\n",
      "Saving side-by-side frames to: /Users/akhidre/pubgit/CS230_T2Motion/frames\n",
      "Saved: /Users/akhidre/pubgit/CS230_T2Motion/frames/sbs_frame_1_20251203-101146.png\n",
      "Saved: /Users/akhidre/pubgit/CS230_T2Motion/frames/sbs_frame_10_20251203-101146.png\n",
      "Saved: /Users/akhidre/pubgit/CS230_T2Motion/frames/sbs_frame_100_20251203-101146.png\n",
      "Finished side-by-side static frame generation.\n"
     ]
    }
   ],
   "source": [
    "# Request which frames to export\n",
    "requested_frames = [1, 10, 100]   # modify as needed\n",
    "\n",
    "max_frame = min(pred_world.shape[0], gt_motion_fixed.shape[0]) - 1\n",
    "safe_frames = [f for f in requested_frames if 0 <= f <= max_frame]\n",
    "\n",
    "print(\"Requested frames:\", requested_frames)\n",
    "print(\"Using frames:\", safe_frames)\n",
    "\n",
    "# Output directory\n",
    "static_dir = f\"/Users/akhidre/pubgit/CS230_T2Motion/frames\"\n",
    "os.makedirs(static_dir, exist_ok=True)\n",
    "print(\"Saving side-by-side frames to:\", static_dir)\n",
    "\n",
    "\n",
    "# Utility: plot one skeleton using SAME orientation as animation\n",
    "def plot_skeleton(ax, frame_pose):\n",
    "    \"\"\"\n",
    "    ax: matplotlib 3D axis\n",
    "    frame_pose: (22,3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fixed axis limits (same as your animation)\n",
    "    ax.set(xlim3d=(-1, 3), xlabel='X')\n",
    "    ax.set(ylim3d=(-1, 1), ylabel='Y')\n",
    "    ax.set(zlim3d=(-1, 1), zlabel='Z')\n",
    "\n",
    "    # Draw skeleton edges\n",
    "    for (i, j) in edges:\n",
    "        x = [frame_pose[i, 0], frame_pose[j, 0]]\n",
    "        y = [frame_pose[i, 1], frame_pose[j, 1]]\n",
    "        z = [frame_pose[i, 2], frame_pose[j, 2]]\n",
    "\n",
    "        # SAME orientation as animation (z, x, y)\n",
    "        ax.plot([z[0], z[1]], [x[0], x[1]], [y[0], y[1]], 'b-', linewidth=2)\n",
    "\n",
    "\n",
    "# Save one side-by-side figure\n",
    "def save_sbs_frame(frame_idx, gt_pose, pred_pose, save_dir):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # Left: GT\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    ax1.set_title(f\"GT â€” Frame {frame_idx}\")\n",
    "    plot_skeleton(ax1, gt_pose)\n",
    "\n",
    "    # Right: Predicted\n",
    "    ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    ax2.set_title(f\"PRED â€” Frame {frame_idx}\")\n",
    "    plot_skeleton(ax2, pred_pose)\n",
    "\n",
    "    # Save high-resolution PNG\n",
    "    out_path = os.path.join(save_dir, f\"sbs_frame_{frame_idx}_{time_stamp}.png\")\n",
    "    plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "\n",
    "\n",
    "# Generate all requested frames\n",
    "for f in safe_frames:\n",
    "    save_sbs_frame(\n",
    "        f,\n",
    "        gt_motion_fixed[f],   # GT frame\n",
    "        pred_world[f],        # Predicted frame\n",
    "        static_dir\n",
    "    )\n",
    "\n",
    "print(\"Finished side-by-side static frame generation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315a718",
   "metadata": {},
   "source": [
    "### Troubleshoot/Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9cbdef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT motion shape: (200, 22, 3)\n",
      "Pred motion shape: (200, 22, 3)\n",
      "==== frame 0 ====\n",
      "GT   frame   0 | center [-0.02864403  0.96302295  0.02325513] | min [-0.44450423  0.02759657 -0.21121013] | max [0.21953316 1.5654216  0.27112737] | scale 0.958\n",
      "Pred frame   0 | center [0.00153125 0.19702187 0.01724212] | min [-0.05245192  0.00887568  0.00560129] | max [0.0564133 0.3203984 0.031524 ] | scale 0.192\n",
      "\n",
      "==== frame 1 ====\n",
      "GT   frame   1 | center [-0.03720071  0.95801127  0.03821034] | min [-0.45528355  0.02489816 -0.20703632] | max [0.20616621 1.5639188  0.26679406] | scale 0.953\n",
      "Pred frame   1 | center [0.00263812 0.43550768 0.03544001] | min [-0.1042376   0.02060861  0.01222342] | max [0.11616597 0.7143726  0.06515045] | scale 0.421\n",
      "\n",
      "==== frame 5 ====\n",
      "GT   frame   5 | center [-0.05281306  0.92984957  0.11754942] | min [-0.3982659   0.02329702 -0.04673944] | max [0.1719622 1.5448701 0.2994337] | scale 0.916\n",
      "Pred frame   5 | center [0.00562967 0.87676024 0.07665091] | min [-0.20799063  0.03914583  0.03673457] | max [0.22424278 1.456004   0.12365127] | scale 0.849\n",
      "\n",
      "==== frame 10 ====\n",
      "GT   frame  10 | center [-0.03290682  0.9064513   0.24724287] | min [-0.25064433  0.02472582  0.07713679] | max [0.16833325 1.5304339  0.51395994] | scale 0.933\n",
      "Pred frame  10 | center [0.00602078 0.9190809  0.08621389] | min [-0.21739022  0.04023353  0.04496373] | max [0.23192644 1.5298994  0.1361597 ] | scale 0.891\n",
      "\n",
      "==== frame 50 ====\n",
      "GT   frame  50 | center [-0.310685    0.9073925   0.16441862] | min [-0.5222204   0.01687903 -0.05121526] | max [0.14026022 1.533243   0.35303393] | scale 0.900\n",
      "Pred frame  50 | center [0.00767187 0.9292305  0.23430686] | min [-0.21204059  0.04379351  0.19034697] | max [0.228549   1.5468973  0.28553447] | scale 0.897\n",
      "\n",
      "==== frame 100 ====\n",
      "GT   frame 100 | center [ 0.4740179   0.92271334 -0.36916187] | min [ 0.1638121   0.02326456 -0.6308813 ] | max [ 0.7274685   1.5233259  -0.07731667] | scale 0.936\n",
      "Pred frame 100 | center [0.00753244 0.9252419  0.22429958] | min [-0.21027015  0.0437701   0.18071444] | max [0.22655164 1.5396085  0.27493945] | scale 0.892\n",
      "\n",
      "==== frame 199 ====\n",
      "GT   frame 199 | center [-0.37656918  0.90382785  0.24599129] | min [-0.631577    0.02122619 -0.03423345] | max [0.04445738 1.5070186  0.53054214] | scale 0.937\n",
      "Pred frame 199 | center [0.00688541 0.94425374 0.15484267] | min [-0.21824418  0.04236912  0.11104429] | max [0.23366755 1.5719125  0.20776296] | scale 0.914\n",
      "\n",
      "First 10 GT scales:    [0.94605136 0.94055307 0.93421674 0.9278808  0.9195605  0.91083074\n",
      " 0.90235794 0.8950651  0.91812533 0.9448728 ]\n",
      "First 10 Pred scales:  [0.19871487 0.43515348 0.6345342  0.7625539  0.8346582  0.8722905\n",
      " 0.8926556  0.90276587 0.9073358  0.91131276]\n",
      "Mean GT scale: 0.9389654 Mean Pred scale: 0.91169417\n"
     ]
    }
   ],
   "source": [
    "# Analyze predicted vs ground-truth motion for sample_idx \n",
    "# 1) Rebuild the ground-truth motion for the same sample_idx\n",
    "gt_motion_raw = motions_test[sample_idx]  # this is an object array (T, 22, 3)\n",
    "gt_motion_fixed = pad_or_truncate_motion(gt_motion_raw, target_len=MOTION_LEN)\n",
    "\n",
    "if USE_NORMALIZATION:\n",
    "    # If you ever turn normalization back on, you need to denormalize both\n",
    "    gt_motion_real = denormalize_motion(gt_motion_fixed, mean, std)\n",
    "    pred_motion_real = denormalize_motion(pred_3d, mean, std)\n",
    "else:\n",
    "    gt_motion_real = gt_motion_fixed.astype(np.float32)\n",
    "    pred_motion_real = pred_real.astype(np.float32)  # already float32\n",
    "\n",
    "print(\"GT motion shape:\", gt_motion_real.shape)\n",
    "print(\"Pred motion shape:\", pred_motion_real.shape)\n",
    "\n",
    "\n",
    "# 2) Simple per-frame stats to see \"scale\" and position\n",
    "def motion_frame_stats(name, motion, frame_idx):\n",
    "    f = motion[frame_idx]                 # [22, 3]\n",
    "    min_xyz = f.min(axis=0)\n",
    "    max_xyz = f.max(axis=0)\n",
    "    center = f.mean(axis=0)\n",
    "    # approximate skeleton scale = max distance from the frame center\n",
    "    scale = np.linalg.norm(f - center, axis=1).max()\n",
    "    print(f\"{name} frame {frame_idx:3d} | \"\n",
    "          f\"center {center} | min {min_xyz} | max {max_xyz} | scale {scale:.3f}\")\n",
    "\n",
    "for fi in [0, 1, 5, 10, 50, 100, 199]:\n",
    "    print(\"==== frame\", fi, \"====\")\n",
    "    motion_frame_stats(\"GT  \", gt_motion_real, fi)\n",
    "    motion_frame_stats(\"Pred\", pred_motion_real, fi)\n",
    "    print()\n",
    "    \n",
    "\n",
    "# 3) Track how skeleton scale changes over time\n",
    "def skeleton_scale_over_time(motion):\n",
    "    # assume joint 0 is root\n",
    "    root = motion[:, 0, :]                             # [T, 3]\n",
    "    dists = np.linalg.norm(motion - root[:, None, :], axis=2)  # [T, 22]\n",
    "    return dists.max(axis=1)                           # [T]\n",
    "\n",
    "gt_scale = skeleton_scale_over_time(gt_motion_real)\n",
    "pred_scale = skeleton_scale_over_time(pred_motion_real)\n",
    "\n",
    "print(\"First 10 GT scales:   \", gt_scale[:10])\n",
    "print(\"First 10 Pred scales: \", pred_scale[:10])\n",
    "print(\"Mean GT scale:\", gt_scale.mean(), \"Mean Pred scale:\", pred_scale.mean())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".My_HumanML3D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
